---
layout:     post
title:      "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"
subtitle:   "paper reading"
date:       2018-10-10 10:45:00
author:     "Dawei"
header-img: img/planet_earth_4k.jpg
tags:
    - paper reading
---
<br>许多NLP task都可以看成question answer(QA)问题，情感分析，词性标注，都可以当作一个问题问出来，然后模型给出答案。论文给出Dynamic Memory Networks，可以在一定程度上看成NLP的通用模型。论文主要分为四个模块：输入模块、问题模块、Episodic Memory模块、回答模块。接下来我会依次介绍这四个模块。<br/>
<br>输入模块，这里分为单个句子和多个句子。单个句子将每个word看成一个单元，用GRU处理，输出hidden state即可。对于多个句子，则将每个句子看成一个单元，将全部句子concatenate一起，在每一个句子最后插入一个结束符号，然后用GRU处理，这样每个结束符号的hidden state作为该句的input。<br/>
<br>问题模块，这一个模块很简单，问题通常是一句话，直接将句子的单词通过GRU处理，直接用最后的hidden state代表这个问题。<br/>
<br>Episodic Memory模块，ok，核心点来了，情景记忆模块，中文翻译可能不是很准确。就像我们根据一段话来回答问题的时候，我们会根据问题先看一遍，找到和问题最相关的句子，获得信息，但是这个信息可能不是最终答案，我们需要这个信息再次查找context，找到更进一步的信息，直到找到答案。就像论文Fig. 3,问题是足球在哪里，我们扫描第一遍，首先发现足球在John那里，第二遍找John在哪，发现John在hallway，这就找到答案了。每次根据上一次memory和新获得的信息用GRU更新memory，初始的memory设置为问题模块的输出。这里首先说一下新信息这怎么获得的，每一次迭代，首先用attention计算每个input的权重，attention依靠input token（输入模块的输出），上一层的memory，问题模块的输出，将input token与memory和问题输出各种交互，最后输出一个scalar score，代表这个input在这一层的权重。之后就是memory更新阶段，不同于之前的attention机制，直接将权重和input相乘得到最后，论文还是使用了GRU进行一步步的处理，每次更新的时候根据权重确定更新程度，这样最后的hidden state作为新获得的信息来更新memory。关于迭代次数，论文指出可以在input加入一个结束标记，如果关于这个标记的权重超过一定的阈值，就可以终止了。这样，我们就能够得到最终的memory。<br/>
<br>回答模块，也是很简单的GRU模块，初始的hidden state设置为Episodic Memory模块输出的memory，输入为上一步的输出和问题模块的输出，这样一步步输出直到EOS。<br/>
<br>上面的都是对问题给出一个全局的回答，如果要对input sentence每个word做出处理，例如POS问题，这样每个word在每层的hidden state不是最后的hidden state，而是在word这一步的hidden state。<br/>
<br>模型在QA、语义分析、POS tag等任务都得到很好的效果。并且对每次迭代每个input权重都可以得到很好的可视化效果，让我们很清晰地看到每次迭代对那些input更注重，很有趣的处理方式。<br/>