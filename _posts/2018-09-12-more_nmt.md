---
layout:     post
title:      "CS224N Gated recurrent units and further topics in NMT"
subtitle:   "lecture study"
date:       2018-09-12 15:03:00
author:     "Dawei"
header-img: img/nlp.jpg
tags:
    - lecture study
---

<br>这篇博客主要针对上一篇"CS224N neural machine translation and models with attention"遗留的问题进行展示和解决，在Deep Learning领域，关于准确度和计算量是个tradeoff的结果，而具体到NMT领域，则是vocabulary size大小的选择问题，太大能够将更多的单词划进模型里，从而出现更少的Unknown words（UNK），但是带来更大的计算量，同时即使增大vocabulary size，语料库不大的话，导致rare words变多，这些单词无法得到足够的训练，也是白搭。为了解决这个问题，大致有三个思路，第一个针对softmax进行计算量缩减，但是无法解决rare words问题，第二个是利用context information，从source language直接选取word copy过去，第三个是将word分解成更小的模块，比如character或者subword unit。本文选取"Pointing the Unknown Words","On Using Very Large Target Vocabulary for Neural Machine Translation”，"Neural Machine Translation of Rare Words with Subword Units","Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"这四篇论文展开。<br/>

## On Using Very Large Target Vocabulary for Neural Machine Translation
<br>论文首先提出在现有NLP模型里，large vocabulary和现有的计算能力是矛盾项。论文提出在反向传播输出最终概率softmax那一块，在计算导数时候，只选取一部分单词V'来模拟整体V更新，降低计算量（和word2vector那块的negative sampling有点相似，ns是在loss定义的时候只更新一部分），那些数学公式我没太看懂（囧。。论文提出一种均匀分开V'的方法，计算输出概率只在V'内计算。作者还提几个trick，我就不细说了。<br/>

## Pointing the Unknown Words
<br>论文为了解决unk和rare word问题，提出首先判断是用target word还是source word，这对应两个softmax layer，一个预测在原文的位置，一个和之前的attention机制相同来预测target word。具体公式我就不列了，只是在原有attention的模型加上判断参数和source word选取。但是这篇论文对于unk、rare word采取这种方式，只对于那些原文中能够直接摘取的单词有用，一般是地名、人名或者专业名词，但是那些compose word或者只是word没得到足够训练，效果会很差。<br/>

## Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models
<br>论文通过引入character机制在在一定程度有效解决unk，rare word问题。通过pretrain对train corpus出现的unk和rare word拆解为character represention，在每一轮training时候，先将batch sentence需要被分解character先喂进character专用的lstm网络，然后进行word层次的lstm网络，具体见论文figure 1，这也是为了training效率，理论分析每个character lstm的initial hidden state用上一步输出的hidden state更好，但是拉低训练速度，索性全部直接用zero vector初始化了。对于output也有这方面的处理，输出时候，也使用attention机制，如果有unk，喂进decoder character lstm里面输出character composed word，这里有个trick，也是为了训练速度，在下一步word output时候，其实可以用这一步character decoder的final hidden state，但是需要等待这个网络运行完毕，这样的话，就直接用unk token作为输入输进下一步了。论文给出的策略是先利用beam search运行word decoder不管unk，找到最好的word层面的output，之后再对unk进行处理。<br/>

## Neural Machine Translation of Rare Words with Subword Units
