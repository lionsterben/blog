---
layout:     post
title:      "CS224N CNN in NLP"
subtitle:   "lecture study"
date:       2018-09-28 16:47:00
author:     "Dawei"
header-img: img/nlp.jpg
tags:
    - lecture study
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
<br>Convolutional Neural Network经常被用在compute vision里，用来逐步提取局部特征，直到整体，在nlp里面也可以用CNN解决一些问题。众所周知，CNN一大特点，就是可以并行化计算，比RNN序列化模型快到不知道哪里去了。而且针对RNN模型还有个问题就是无法捕捉没有前置文字的短语，换句话说，就是在encode时候，只能从开始到结束，无法捕捉中间环节，而CNN通过不同size的filter非常容易解决这个问题。今天通过“Convolutional Neural Networks for Sentence Classification”，“A Convolutional Neural Network for Modelling Sentences”，“QUASI-RECURRENT NEURAL NETWORKS”介绍CNN在NLP里的运用。<br/>

## Convolutional Neural Networks for Sentence Classification
<br>这篇论文结构非常简单，通过简单设计只有一层convolution layer的CNN模型，修改下超参数，结果显示可以达到非常优秀的效果。首先使用pre-trained的word vector将sentence转换为n*k的矩阵（n是单词个数，k是embedding维度），然后选取多个filter（维度为$R^{hk}$）,h是此filter window size（相当于h-gram）。通过不同window size的filter可以探测不同长度的短语，即使相同的size也可以探测不同的特征。在conv layer之后接上max pooling，表示反应最大的短语，即是否存在某个特征。最后全连接用softmax输出即可。CNN固有的并行化计算优势对比传统RNN体现地淋漓尽致，并且保留了一定的语序信息。<br/>
<br>论文还提到几个trick，第一个是输入采用两种word vector，都是pretrained，只不过一个固定，一个在训练时候可变。第二个trick是采用dropout防止过拟合，这里就不展开说了。第三个很少见，在参数w进行clip，一般是gradient clip。论文显示这个模型在多个任务（情感分析和问题分类）达到state of art效果。有个点值得说一下，trick里提到的双通道输入数据，在数据量少的时候，表现不会比static word vector好，因为数据量少，导致许多单词训练不充分或者根本没有训练，导致原本向量空间接近的word vector训练后发生偏移，反而降低泛化能力。<br/>

## A Convolutional Neural Network for Modelling Sentences
<br>这篇论文模型和第一篇相似，但有几个CNN用在NLP的点需要注意。在确定filter kernel size之后如何做conv，先假定s为sentence长度，m是kernel window size。论文提到两种，narrow conv，没有padding，每m个word生成一个，总计生成$R^{s-m+1}$向量。wide conv，通过padding zero vector，生成$R^{s+m-1}$。<br/>
<br>这篇论文采用的conv和上一篇不同，上一篇直接对n-gram所有维度一起求和，所以filter才是$R^{hk}$。这篇论文是对word不同维度分别做卷积，所以filter维度是$R^{d*m}$(输入数据维度是$R^{d*s}$，s为sentence长度，d是word vector维度)。在max-pooling，论文也做了改进，取top-k max值按照每个维度原有顺序筛选出来，从而保留更多的信息和一定的相对顺序信息。这个模型一般是多个conv layer和pool layer组成block多个block叠加在一起，在最上层的pool-layer的k值是要固定的，否则之后的全连接层的参数确定不了，而之前的k值则可以根据句子长度确定，具体公式见论文。对于上一篇论文也有这个问题，即不同长度的句子如何处理，上一篇在conv层也是不固定的，每个conv出来的feature map和句子长度有关，但是通过max-pool将不同长度的feature map全部变成一个数。论文在每个block pooling之后加了一个non-linearity层。论文还有一些处理的细节，比如每层多个feature map和fold layer，不是重点，我就不展开说了。<br/>
<br>关于这篇论文算是上篇论文的复杂版，卷积不局限于单个word，而是对word每个维度都进行，max-pool也同理。如果说上一篇论文max-pool选出最有影响的n-gram，这篇是对n-gram每个维度都选出最有影响的top k个值，可以这样解释吗？反正论文显示效果是不错啦。<br/>

## QUASI-RECURRENT NEURAL NETWORKS
<br>这篇论文比较新，结合RNN和CNN的有点提出的一个模型QRNN。模型首先通过conv layer并行化计算Z,F,O,类似lstm里面中间状态$c_t$,$f_t$,$o_t$，但是是一下全部取出来,也就是说每一步的z,f,o不需要上一步输出的值，只依靠不同window size的输入数据，就很快。然后喂入fo-pool层，这一层需要串行计算了，求出每一步的$h_t$，$c_t$,然后作为输入喂到下层conv layer。模型大致就是这样，我的理解就是论文将原始lstm分开了，需要矩阵乘法的地方，即计算z,f,o利用卷积，放弃使用上一步状态，只使用输入数据，达到并行化效果，但是在计算$h_t$，$c_t$这些只需要element-wise的时候，按照串行执行，保留全部的语序信息，可以说是结合两种模型的优点了，从计算复杂度角度也可以说得通，对于两个$R^{n*n}$的矩阵相乘，复杂度是O($n^3$)，而element wise是O($n^2$)，计算量大的并行化处理可以大大节约时间。<br/>
<br>在这里总结一下，CNN虽然具有并行化处理的优势，但是经过max-pool，会丢失很多语序信息，这也是CNN擅于处理NLP里面的分类问题（分类很很多时候不需要全部语序信息，只需要探测是否含有这个特征即可，或者几个重要的特征组合），但是在处理PoS Tagging或Entity Extraction，就比较困难，而且将句子里面相邻单词组合组成phrase，有时候不会很make sense，因为语言中phrase可能会被多个单词隔开。关于CNNd的缺点，我的理解，主要是在pooling层丢失全序信息。<br/>