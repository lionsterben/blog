---
layout:     post
title:      "Generative Adversarial Nets"
subtitle:   "paper reading"
date:       2018-10-09 15:04:00
author:     "Dawei"
header-img: img/planet_earth_4k.jpg
tags:
    - paper reading
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
<br>GAN作为深度学习生成模型领域新秀，从2014年从Ian goodfellow提出后，逐渐受到人们关注成为生成模型的主流，[这里](https://github.com/nightrome/really-awesome-gan)有一份别人梳理的GAN领域的论文，有兴趣的小伙伴可以看一看。这篇博客主要讲述原始GAN论文和Wasserstein GAN这两篇论文。<br/>

# Generative Adversarial Nets
<br>生成对抗网络，顾名思义分为两块，生成和对抗网络。生成网络负责生成尽可能与真实样本相似分布的结果而判别网络（对抗网络）则尽可能将生成的样本和真实样本区分开来，通过这两个网络的不断博弈，从而提高生成网络的能力，最终生成和真实样本相同分布的结果。<br/>
<br>总而言之，这是一个典型的最大最小问题，我们定义一个value function（衡量正负样本是否被有效区分开的程度），生成器尽可能减小value（表示区分不出来正负样本），判别器尽可能增大value（使生成样本在判别器里尽可能判断为真实样本），论文的Fig. 1很形象地解释了这一过程。关于实现算法方面，首先对真实样本进行抽样，然后从noise先验分布里抽样，哦，忘了解释生成器和判别器的输入输出了，生成器输入一些noise，喂进生成器神经网络，生成生成样本，然后将这批生成样本和真实样本喂进判别器里，让它对样本所属类别做出判断。论文算法首先训练判别器，固定生成器参数，判别器会多训练几轮，由超参数决定，尽量生成最优形式。然后固定判别器，训练生成器。<br/>
<br>论文给出了数学证明，说明全局最优状态是生成分布等于真实分布。具体的数学证明过程这里我就不贴出来了，简单讲一下过程。在固定生成器参数的时候，判别器最优化的形式是$\frac{P_{data}(x)}{P_{data}(x)+P_{g}(x)}$，$P_{data}(x)$是数据真实分布，$P_{g}(x)$是生成分布。将这个结果带入value function，这时候生成器需要优化的value function可以看成$P_{data}(x)$和$P_{g}(x)$的JS距离加上一个常数，而JS距离是非负的，如果想要达到最小，只有在$P_{data}(x)$和$P_{g}(x)$相等的时候，也就是为0。论文算法对于判别器多训练几次的原因也在于此，希望每次判别器都达到最优形式，从而生成器才可以达到收敛。注意这里会出现问题，会在下一节解释，在开始的时候，如果判别器训练太好，反而会导致生成器无法获得梯度，这是JS距离本身的问题。<br/>

# Wasserstein GAN
<br>Wasserstein GAN（以下简称WGAN）指出原始GAN的问题：1.极难训练，很难收敛 2.没有一个明确的指标标记训练程度。论文首先在数学形式上指出原始GAN loss的问题，然后提出Wasserstein距离，改变loss定义，改了几点算法就可以解决这两个问题。这篇论文非常hardcore，通篇数学证明，我就用文字大致解释一下。<br/>
<br>WGAN首先解释GAN loss问题，在上一节也大致提到了，在判别器训练到最优的情况下，生成器loss可以变成JS距离，这一点上一节提到过，在生成分布和真实分布没有重叠（这里应该有严格的数学定义，但是方便理解，就直接说重叠了）的情况下，JS距离会恒定为一个常数,这样生成器根本不可能通过梯度下降来减小JS距离来缩小两个分布的差异，换句话说，只要两个分布没有重叠，不管它们远在天边还是近在眼前，JS距离都是一样的。而在训练初期，没有重叠的概率是非常大的，也就是说训练刚开始就会陷入停顿。<br/>
<br>在这篇论文提出之前，也有方法可以解决不能收敛的问题，既然开始的时候没有重叠，我们就加噪声强制让它们有重叠，之后逐渐对噪声去火，反正它们本身也有重叠了，这样就能够减小JS距离来拉近真实分布和生成分布，但是还是有个问题，还是没有一个指标来衡量训练进程。<br/>
<br>如果说JS距离不能很好的表示分布距离，那么还有其它距离表示分布之间的差异，而且不具有JS的距离的缺点吗？很好的是，有的。Wasserstein距离就是一个，在两个分布即使没有重叠的时候，它在衡量两个分布距离时不是突变的，而是连续的，这样就能够使用梯度下降，减小wasserstein距离，来拉近两个分布。论文给出如何计算这个距离，我就不贴出来了，恰好这个距离是一个满足一定条件的函数所有值的上界，这可以用一个具有一定限制性的神经网络表示，简直perfect。然后原始GAN算法的实际意义就变了，判别器变成求wasserstein距离，通过最大化value function（即wasserstein距离的表示形式）来获得当前两个分布的距离，而生成器则是较小这个value function来拉近两个分布。这个过程不存在无重叠时，生成器无梯度的问题，而且每次判别器loss可以代表wasserstein距离即分布之间相似程度，而可以直接看出训练进度。<br/>